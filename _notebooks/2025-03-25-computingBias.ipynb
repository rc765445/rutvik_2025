{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title: Computing Bias Homework\n",
    "description:  Homework for Computing Bias, Johan, Daksha, Thomas \n",
    "type: issues \n",
    "comments: true\n",
    "courses: { csp: {week: 1} }\n",
    "comments: True\n",
    "sticky_rank: 1\n",
    "permalink: /Computing Bias/\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### POPCORN HACK #1 \n",
    "\n",
    "\n",
    "- Example: Google Translate\n",
    "- Bias: Gender Stereotyping in Translations\n",
    "\n",
    "#### Who is Affected?\n",
    "- Users who rely on Google Translate for language conversion, particularly in gender-neutral languages like Turkish, Finnish, or Hungarian, can experience biased outputs. For example, when translating sentences from Turkish (which uses a gender-neutral pronoun “o”), Google Translate often assigns genders based on stereotypes:\n",
    "\n",
    "- “O bir doktor.” → “He is a doctor.”\n",
    "\n",
    "- “O bir hemşire.” → “She is a nurse.”\n",
    "\n",
    "- This reinforces gender stereotypes, affecting perceptions of professions and roles across different languages.\n",
    "\n",
    "#### Potential Cause of This Bias:\n",
    "The model learns from vast amounts of text data collected from the internet, where societal biases are already present. If the majority of training data associates doctors with men and nurses with women, the algorithm reflects and amplifies these patterns, rather than making neutral or balanced translations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POPCORN HACK #2\n",
    "\n",
    "\n",
    "- I was in a public restroom, rushing to wash my hands, but no matter how many times I waved my hand under the automatic soap dispenser, nothing happened. I tried again and again, but it just wouldn’t work. \n",
    "- Then, someone else walked up, and the soap came out instantly for them. It was frustrating and honestly a little embarrassing. \n",
    "- Later, I found out that some sensors don’t always detect darker skin tones well because they weren’t tested on a diverse range of people. It made me feel like I wasn’t even considered. If companies tested these devices on all skin tones, it would make a huge difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POPCORN HACK #3\n",
    "\n",
    "- If I were designing a fitness tracking app, I’d want to make sure it works for everyone, not just people who are already super active. A lot of fitness apps focus on intense workouts, step counts, or burning calories, which can make people with different abilities, ages, or health conditions feel left out. Someone recovering from an injury or living with a chronic illness might not be able to hit 10,000 steps a day, and that shouldn’t make them feel like they’re failing.  \n",
    "\n",
    "- To make the app more inclusive, I’d let users set goals that actually fit their needs—whether that’s improving flexibility, building strength, or just staying active in any way that works for them. I’d also include workouts for different ability levels, like seated exercises or low-impact movement. And instead of just tracking steps or calories, the app could celebrate consistency, progress in mobility, or even just feeling better overall. Fitness should be for everyone, and the app should reflect that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digital Tool: YouTube  \n",
    "\n",
    "#### Potential Bias: \n",
    "YouTube’s recommendation algorithm tends to push users toward certain types of content based on their past viewing habits. This can create “filter bubbles,” where users are repeatedly shown similar videos, limiting exposure to diverse perspectives. Additionally, some creators from marginalized communities have reported that their content gets demonetized or isn’t promoted as much, making it harder for them to reach a wider audience.  \n",
    "\n",
    "#### Cause of Bias: \n",
    "This bias likely comes from the way YouTube’s algorithm is designed—it prioritizes watch time and engagement over diversity. If a user primarily watches one type of content, the algorithm assumes they only want more of the same. Additionally, since YouTube’s monetization system relies on automated content moderation, it may unfairly flag videos from certain creators due to keyword associations or biased training data.  \n",
    "\n",
    "#### Solution:  \n",
    "YouTube could introduce a more balanced recommendation system that intentionally diversifies suggested content. For example, it could mix in videos from different viewpoints, lesser-known creators, or underrepresented communities alongside a user’s usual content. Additionally, improving transparency in content moderation and involving diverse testers in algorithm development could help ensure fairer treatment of all creators."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
